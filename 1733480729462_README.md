# vineeth-project-
# On the Effectiveness of Adaptive Lossless Image Compression Employing Huffman Encoding

## Project Overview
This project integrates an adaptive lossless image compression algorithm utilizing Huffman Encoding. The first aim is to provide a fast and accurate method of compressing grayscale images where image quality is preserved during both encoding and decoding phases.

## Objectives
Create a flexible lossless image compression and decompression approach by Huffman Encoding.
As used for grayscale images, improve the rate of achieving the ideal data compression factor without significant loss of data quality.
- Compare the compression ratio ad and the encoding/decoding durations, and the precision of the decompression.

## Key Concepts
- Huffman Encoding: An example of lossless coding techniques that assigns short code to frequently occurring symbols (pixel values) and long code to less frequently occurring symbols.
- Adaptive Huffman Encoding: An image encoding technique that is an enhanced version of the Huffman algorithm and the coding tree is changed as the image is being encoded.

## Methodology
1. Encoding Process:
   The image is converted into grayscale and array list and then this array list is converted into numpy array format.
   The number of times of occurrence of specific pixel values is counted, Huffman tree based on the priority queue is then developed.
   The picture elements are exchanged for Huffman codes of the same pixel.

2. Decoding Process:
   The compressed bitstream is then, decoded utilizing Huffman tree.
   Pixel values are then obtained from the binary bit stream and the original image is reconstructed without any loss.

## Performance Evaluation
- Compression Ratio: The average compression ratio of algorithm found to be of **1.16 : 1*. This is very much a function of the image content where by the simpler images are more easily compressed than others.
- Encoding Time: More specifically, the encoding time is somehow dependent with the intricacy and size of the image in question. The building of the Huffman tree is a computational process, or in other words it is costly.
- Decoding Time: Decoding can be done much faster as all you are giving is just the encoded text and the only thing you have to do is navigate through the constructed Huffman tree.
- 
## How to Run

- Navigate to the project folder where the script is located.
- Run the Python script: In the google collab
   ```
      1733072840478_VINEETH_US.ipynb
   ```

- From there you can insert the data set
     Kamaljp_cifar_500.zip
- Copy the path of the data set and paste it in the defined paths at zip_file_path
- Then run the code 
  
## Input Requirements
- Supported Formats: .jpg, .png
- Input Image Size: The tool supports any image size, but very large images may take longer to process.
- Aspect Ratio: The program maintains the aspect ratio when resizing the image.
- **Supported Formats**: JPG, PNG.  
- **Recommended Size**: Images should ideally be under 5MB to ensure smooth performance.

## Project Structure

```
.
├── 1733480729462_README.md # Project documentation
├── vineeth.py # Main Python script
```

## Expected Output
- After loading the images, the images will get processed.
- You will get the original size and encoded size of the images
- After processing the images you will get the compression ratio.
- Then you will get the Encoding Time and  Decoding Time of every image.
- And finally you will get the plot which is the original size vs the encoded size for the processed images.

## Graphical Results
Curves of the compression ratios were introduced indicating that the amount of detail in the images had a determinant relationship to the compression ratio achieved.
This analysis concluded that an improved compression ratio was attained for images with large homogeneous areas than for complex ones with high entropy.

## Decompression Accuracy
The algorithm effectively separates out the packaged images so that no information is lost as with more complicated or diverse images.
The quality of its decompressed images is then checked at a meta-pixel level to ensure that the compression is entirely lossless.

## Future Work
- They should consider studying more about parallel processing or even the use of GPU in an effort of reducing the encoding time.
Research other lossless algorithms that are out there among them being Arithmetic Coding or Lempel–Ziv–Welch (LZW).
Adding an interface makes it easier to use for non-technical personnel, especially as they only interact with Graphical User Interface (GUI).
Figure out how to adapt it into handling of color images and bigger sample sizes.
